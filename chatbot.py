# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dxNHFPSDWj4Qq-iKsLWwUx4pPglNk6gS
"""

!pip -q install langchain
!pip -q install bitsandbytes accelerate transformers
!pip -q install datasets loralib sentencepiece
!pip -q install pypdf
!pip -q install sentence_transformers

!pip -q install openai
!pip -q install tiktoken
!pip -q install unstructured
!pip install tokenizers
!pip install faiss-cpu
!pip install xformers
!pip install pinecone-client

from langchain.document_loaders import UnstructuredURLLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.vectorstores import Pinecone
import pinecone
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
from langchain import HuggingFacePipeline
from huggingface_hub import notebook_login
import textwrap
import sys
import os
import torch
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

os.environ['OPENAI_API_KEY']='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'
URLs=[
    'https://www.radheradheje.com/culture-bhagavadgita-quotes-100-best-bhagavad-gita-quotes/'
]
loaders=UnstructuredURLLoader(urls=URLs)
data=loaders.load()
data
len(data)

os.environ['OPENAI_API_KEY']='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'
URLs=[
    'https://www.radheradheje.com/culture-bhagavadgita-quotes-100-best-bhagavad-gita-quotes/'
]
loaders=UnstructuredURLLoader(urls=URLs)
data=loaders.load()
data
len(data)

#embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
embeddings=HuggingFaceEmbeddings()
query_result = embeddings.embed_query("Hello world")
len(query_result)

vectorstore=FAISS.from_documents(text_chunks, embeddings)

llm=ChatOpenAI()
llm
notebook_login()

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf",
                                          use_auth_token=True,)

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf",
                                             device_map='auto',
                                             torch_dtype=torch.float16,
                                             use_auth_token=True,
                                              load_in_8bit=True,
                                              #load_in_4bit=True
                                             )
pipe = pipeline("text-generation",
                model=model,
                tokenizer= tokenizer,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                max_new_tokens = 512,
                do_sample=True,
                top_k=30,
                num_return_sequences=1,
                eos_token_id=tokenizer.eos_token_id
                )
llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})
llm.predict("Please provide a concise summary of the Book Alchemist")

chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())


result=chain({"question": "Who lives in wisdom"}, return_only_outputs=True)


result['answer']

chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())


result=chain({"question": "What can we offer to Lord"}, return_only_outputs=True)


result['answer']